<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research | Ugo Labbé</title>
    <link rel="stylesheet" href="assets/styles.css">
</head>
<body>

<header>
    <div class="header-top">
        <a href="https://www.linkedin.com/in/ugo-labb%C3%A9-03aa341b8/" target="_blank" class="profile-link" title="Ugo Labbé on LinkedIn">
            <img src="assets/linkedin pfp.png" alt="Ugo Labbé profile" class="profile-pic" style="width:72px;height:72px;max-width:72px;">
        </a>
        <div class="header-title">
            <h1>The Science of Multi-Fidelity Modeling</h1>
            <p class="header-position">How I use Gaussian Processes to optimize industrial processes prediction and uncertainty quantification at Michelin</p>
        </div>
    </div>
</header>

<nav>
    <a href="index.html">Home</a>
    <a href="projects.html">Academic projects</a>
</nav>

<section>
    <h2>The Problem: Expensive Reality</h2>
    <p>
        In industrial manufacturing, like tire production at Michelin, we often face a trade-off. We can obtain "High-Fidelity" data which are extremely accurate but are scarce or hard to get, and "Low-Fidelity" approximations which are fast but lack precision. 
        My research aims to combine these sources to get the best of both worlds: <strong>accuracy and speed.</strong>
    </p>

    <div class="concept-card">
        <h3>1. The Foundation: Co-Kriging</h3>
        <p>
            Based on the pioneering work of <strong>Kennedy & O’Hagan (2000)</strong>, I use "Co-Kriging." This statistical framework assumes that different levels of data are correlated. 
            It models the "gap" between a simple model and reality as a Gaussian Process, allowing the cheap data to guide the expensive model's predictions.
        </p>
    </div>

    <div class="concept-card">
        <h3>2. Scaling Up: Recursive Modeling</h3>
        <p>
            When dealing with many levels of data (e.g., experiments, physics simulations, and historical logs), standard models become too slow. 
            I implement the <strong>recursive formulation</strong> proposed by <strong>Le Gratiet & Garnier (2014)</strong>. 
            This breaks one massive, complex problem into a sequence of smaller, independent steps, drastically reducing the computational cost without losing predictive accuracy.
        </p>
    </div>

    <div class="concept-card">
        <h3>3. Real-World Complexity</h3>
        <p>
            As highlighted by <strong>Brevault et al. (2020)</strong>, relationships between data sources aren't always linear. 
            My work focuses on handling these complex relationships—such as variable correlation and non-stationarity—ensuring that our models remain reliable even when the physics of the process changes over time.
        </p>
    </div>

    <h2>Selected Bibliography</h2>
    <ul class="ref-list">
        <li><strong>Kennedy & O’Hagan (2000)</strong>: Predicting output from complex codes when fast approximations are available.</li>
        <li><strong>Le Gratiet & Garnier (2014)</strong>: Recursive co-kriging for multi-fidelity experiments.</li>
        <li><strong>Brevault, Balesdent & Hebbal (2020)</strong>: Overview of GP-based multi-fidelity techniques.</li>
    </ul>
</section>

<footer>
    &copy; 2026 Ugo Labbé.
</footer>

</body>
</html>